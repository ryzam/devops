# Backup and Restore RKE2 Clusters

This guide covers comprehensive backup and restore procedures for RKE2 (Rancher Kubernetes Engine 2) clusters. RKE2 provides built-in etcd snapshot functionality and integrates seamlessly with Rancher for centralized backup management. Understanding these procedures is crucial for disaster recovery and maintaining cluster availability.

## Overview

RKE2 clusters store critical data in etcd, including:
- Cluster state and configuration
- Secrets, ConfigMaps, and other Kubernetes objects
- Rancher management data (if integrated)
- Application data and persistent volumes

## Prerequisites

- RKE2 cluster running (see [High-Availability K8s Cluster with RKE2](docs/08-ha-cluster-rke2.md))
- kubectl configured with cluster-admin access
- Sufficient storage space for backups
- Rancher installed (for Rancher-managed backups)

## Method 1: Backup and Restore Using Rancher Management

### Installing Rancher Backup Operator

Rancher provides a backup operator for automated cluster backups:

```bash
# Add Rancher charts repository
helm repo add rancher-stable https://releases.rancher.com/server-charts/stable
helm repo update

# Install backup operator
helm install rancher-backup-crd rancher-stable/rancher-backup-crd \
  --namespace cattle-resources-system \
  --create-namespace \
  --version 3.1.0

helm install rancher-backup rancher-stable/rancher-backup \
  --namespace cattle-resources-system \
  --version 3.1.0
```

### Creating a Backup Using Rancher UI

1. **Access Rancher Dashboard**:
   - Log in to your Rancher instance
   - Navigate to the target cluster

2. **Navigate to Backup & Restore**:
   - Go to **Cluster Tools** → **Backup & Restore**
   - Click **Create** to set up a new backup

3. **Configure Backup Settings**:
   ```yaml
   # Example backup configuration - AWS S3
   apiVersion: resources.cattle.io/v1
   kind: Backup
   metadata:
     name: rke2-cluster-backup-s3
     namespace: cattle-resources-system
   spec:
     resourceSetName: rancher-resource-set
     schedule: "@every 6h"  # Optional: scheduled backups
     retentionCount: 10
     storageLocation:
       s3:
         credentialSecretName: s3-creds
         credentialSecretNamespace: cattle-resources-system
         bucketName: rke2-backups
         region: us-west-2
         endpoint: s3.us-west-2.amazonaws.com
   ```

   ```yaml
   # Example backup configuration - MinIO
   apiVersion: resources.cattle.io/v1
   kind: Backup
   metadata:
     name: rke2-cluster-backup-minio
     namespace: cattle-resources-system
   spec:
     resourceSetName: rancher-resource-set
     schedule: "@every 6h"  # Optional: scheduled backups
     retentionCount: 10
     storageLocation:
       s3:
         credentialSecretName: minio-creds
         credentialSecretNamespace: cattle-resources-system
         bucketName: rke2-backups
         region: us-east-1  # MinIO doesn't use regions, but field is required
         endpoint: minio.example.com:9000
         endpointCA: LS0tLS1CRUdJTi...  # Optional: Base64 encoded CA certificate
   ```

4. **Create Storage Location**:
   - Configure S3-compatible storage (AWS S3, MinIO, or other S3-compatible services)
   - For AWS S3: Create credentials secret
   ```bash
   kubectl create secret generic s3-creds \
     --namespace cattle-resources-system \
     --from-literal accessKey=your-access-key \
     --from-literal secretKey=your-secret-key
   ```

   - For MinIO (S3-compatible object storage):
   ```bash
   kubectl create secret generic minio-creds \
     --namespace cattle-resources-system \
     --from-literal accessKey=minio-access-key \
     --from-literal secretKey=minio-secret-key
   ```

   **MinIO Setup (Optional)**:
   If you don't have MinIO installed, you can deploy it in your cluster:
   ```bash
   # Add MinIO Helm repository
   helm repo add minio https://charts.min.io/
   helm repo update

   # Install MinIO
   helm install minio minio/minio \
     --namespace minio-system \
     --create-namespace \
     --set rootUser=minio-root-user \
     --set rootPassword=minio-root-password \
     --set persistence.size=100Gi \
     --set service.type=ClusterIP

   # Create backup bucket
   kubectl run minio-client --image=minio/mc --rm -i --restart=Never -- \
     mc alias set myminio http://minio.minio-system.svc.cluster.local:9000 minio-root-user minio-root-password && \
     mc mb myminio/rke2-backups
   ```

5. **Execute Backup**:
   - Click **Create** to start the backup process
   - Monitor progress in the Rancher UI

### Restoring from Rancher Backup

1. **Access Backup & Restore Section**:
   - Navigate to **Cluster Tools** → **Backup & Restore**
   - Go to the **Restore** tab

2. **Select Backup Source**:
   - Choose the backup to restore from
   - Configure restore options

3. **Configure Restore Settings**:
   ```yaml
   # AWS S3 Restore
   apiVersion: resources.cattle.io/v1
   kind: Restore
   metadata:
     name: rke2-cluster-restore-s3
     namespace: cattle-resources-system
   spec:
     backupFilename: backup-rke2-cluster-backup-2024-01-01T00-00-00Z.tar.gz
     storageLocation:
       s3:
         credentialSecretName: s3-creds
         credentialSecretNamespace: cattle-resources-system
         bucketName: rke2-backups
         region: us-west-2
         endpoint: s3.us-west-2.amazonaws.com
   ```

   ```yaml
   # MinIO Restore
   apiVersion: resources.cattle.io/v1
   kind: Restore
   metadata:
     name: rke2-cluster-restore-minio
     namespace: cattle-resources-system
   spec:
     backupFilename: backup-rke2-cluster-backup-2024-01-01T00-00-00Z.tar.gz
     storageLocation:
       s3:
         credentialSecretName: minio-creds
         credentialSecretNamespace: cattle-resources-system
         bucketName: rke2-backups
         region: us-east-1
         endpoint: minio.example.com:9000
   ```

4. **Execute Restore**:
   - Click **Restore** to begin the process
   - Monitor the restoration progress

## Method 2: Backup and Restore Using kubectl and etcdctl

### RKE2 Built-in Snapshot Backup

RKE2 provides automated etcd snapshots:

```bash
# Check current snapshot configuration
kubectl get configmap rke2-config -n kube-system -o yaml

# Configure snapshot settings in /etc/rancher/rke2/config.yaml
etcd-snapshot-schedule-cron: "0 */6 * * *"  # Every 6 hours
etcd-snapshot-retention: 10
etcd-snapshot-dir: /var/lib/rancher/rke2/server/db/snapshots
```

### Manual etcd Snapshot Creation

```bash
# Create manual snapshot
kubectl etcd snapshot-save /var/lib/rancher/rke2/server/db/snapshots/manual-snapshot.db

# List available snapshots
kubectl etcd snapshot-list

# Verify snapshot
kubectl etcd snapshot-status /var/lib/rancher/rke2/server/db/snapshots/manual-snapshot.db
```

### Cluster Configuration Backup

```bash
# Backup RKE2 configuration
cp /etc/rancher/rke2/config.yaml /backup/rke2-config-$(date +%Y%m%d).yaml

# Backup certificates and keys
tar -czf /backup/rke2-certs-$(date +%Y%m%d).tar.gz \
  /var/lib/rancher/rke2/server/tls \
  /var/lib/rancher/rke2/server/token

# Backup etcd data directory (if not using snapshots)
tar -czf /backup/rke2-etcd-$(date +%Y%m%d).tar.gz \
  /var/lib/rancher/rke2/server/db/etcd
```

### Kubernetes Resources Backup

```bash
# Backup all cluster resources
kubectl get all --all-namespaces -o yaml > /backup/cluster-resources-$(date +%Y%m%d).yaml

# Backup specific namespaces
kubectl get all -n production -o yaml > /backup/production-namespace-$(date +%Y%m%d).yaml

# Backup persistent volumes
kubectl get pv -o yaml > /backup/persistent-volumes-$(date +%Y%m%d).yaml

# Backup secrets and configmaps
kubectl get secrets,configmaps --all-namespaces -o yaml > /backup/secrets-configmaps-$(date +%Y%m%d).yaml
```

### Restore Using etcd Snapshot

For disaster recovery scenarios:

1. **Stop RKE2 Service**:
   ```bash
   systemctl stop rke2-server
   ```

2. **Restore from Snapshot**:
   ```bash
   # On the first server node
   rke2 server \
     --cluster-reset \
     --etcd-snapshot-file /var/lib/rancher/rke2/server/db/snapshots/snapshot.db \
     --token-file /var/lib/rancher/rke2/server/token
   ```

3. **Start RKE2 Service**:
   ```bash
   systemctl start rke2-server
   ```

4. **Verify Cluster Health**:
   ```bash
   kubectl get nodes
   kubectl get pods --all-namespaces
   ```

### Complete Cluster Restore

For full cluster reconstruction:

1. **Reinstall RKE2**:
   ```bash
   # On first server
   curl -sfL https://get.rke2.io | sh -
   systemctl enable rke2-server
   mkdir -p /etc/rancher/rke2
   cp /backup/rke2-config.yaml /etc/rancher/rke2/config.yaml
   systemctl start rke2-server
   ```

2. **Restore etcd Data** (if not using snapshots):
   ```bash
   systemctl stop rke2-server
   tar -xzf /backup/rke2-etcd.tar.gz -C /
   systemctl start rke2-server
   ```

3. **Restore Certificates**:
   ```bash
   tar -xzf /backup/rke2-certs.tar.gz -C /
   ```

4. **Join Additional Nodes**:
   ```bash
   # On additional servers
   curl -sfL https://get.rke2.io | sh -
   systemctl enable rke2-server
   # Use the same config.yaml
   systemctl start rke2-server

   # On agents
   curl -sfL https://get.rke2.io | INSTALL_RKE2_TYPE=agent sh -
   systemctl enable rke2-agent
   systemctl start rke2-agent
   ```

## Automated Backup Scripts

### Daily Backup Script

```bash
#!/bin/bash
# /usr/local/bin/rke2-daily-backup.sh

BACKUP_DIR="/backup"
DATE=$(date +%Y%m%d_%H%M%S)
RETENTION_DAYS=30

# Create etcd snapshot
kubectl etcd snapshot-save ${BACKUP_DIR}/etcd-snapshot-${DATE}.db

# Backup cluster resources
kubectl get all --all-namespaces -o yaml > ${BACKUP_DIR}/cluster-resources-${DATE}.yaml

# Backup configuration
cp /etc/rancher/rke2/config.yaml ${BACKUP_DIR}/rke2-config-${DATE}.yaml

# Compress backups
tar -czf ${BACKUP_DIR}/rke2-full-backup-${DATE}.tar.gz ${BACKUP_DIR}/*${DATE}*

# Clean up old backups
find ${BACKUP_DIR} -name "*.tar.gz" -mtime +${RETENTION_DAYS} -delete
find ${BACKUP_DIR} -name "etcd-snapshot-*.db" -mtime +${RETENTION_DAYS} -delete

echo "Backup completed: ${BACKUP_DIR}/rke2-full-backup-${DATE}.tar.gz"
```

### Schedule Automated Backups

```bash
# Add to crontab for daily backups at 2 AM
0 2 * * * /usr/local/bin/rke2-daily-backup.sh

# Or use systemd timer
cat > /etc/systemd/system/rke2-backup.service << EOF
[Unit]
Description=RKE2 Daily Backup

[Service]
Type=oneshot
ExecStart=/usr/local/bin/rke2-daily-backup.sh
EOF

cat > /etc/systemd/system/rke2-backup.timer << EOF
[Unit]
Description=Run RKE2 backup daily
Requires=rke2-backup.service

[Timer]
OnCalendar=daily
Persistent=true

[Install]
WantedBy=timers.target
EOF

systemctl enable rke2-backup.timer
systemctl start rke2-backup.timer
```

## Best Practices

### Backup Strategy
- **Regular Backups**: Schedule automated backups at least daily
- **Multiple Locations**: Store backups in multiple locations (local + cloud)
- **Test Restores**: Regularly test backup restoration procedures
- **Retention Policy**: Implement appropriate retention based on compliance requirements

### Security Considerations
- **Encrypt Backups**: Use encrypted storage for sensitive backups
- **Access Control**: Restrict access to backup files and credentials
- **Secure Transfer**: Use secure methods for transferring backups

### Performance Optimization
- **Off-Peak Scheduling**: Run backups during low-usage periods
- **Incremental Backups**: Use incremental backups for large datasets
- **Resource Limits**: Monitor resource usage during backup operations

## Troubleshooting

### Common Backup Issues

**Snapshot Creation Fails**:
```bash
# Check etcd health
kubectl get pods -n kube-system | grep etcd

# Check disk space
df -h /var/lib/rancher/rke2/server/db/

# Check etcd logs
kubectl logs -n kube-system etcd-rke2-server
```

**Rancher Backup Operator Issues**:
```bash
# Check operator status
kubectl get pods -n cattle-resources-system

# Check backup CR status
kubectl get backups -n cattle-resources-system

# View operator logs
kubectl logs -n cattle-resources-system deployment/rancher-backup
```

### Common Restore Issues

**Cluster Won't Start After Restore**:
- Verify snapshot integrity
- Check system resources
- Review RKE2 logs: `journalctl -u rke2-server`

**Nodes Won't Join**:
- Verify token validity
- Check network connectivity
- Ensure certificates are valid

**Applications Not Working**:
- Check persistent volume restoration
- Verify secrets and configmaps
- Review application logs

## Monitoring Backup Health

### Backup Metrics

```bash
# Check etcd snapshot status
kubectl get configmap rke2-etcd-snapshots -n kube-system -o yaml

# Monitor backup cron jobs
kubectl get cronjobs -n cattle-resources-system

# Check backup storage usage
kubectl get backups -n cattle-resources-system
```

### Alerting Setup

Configure alerts for backup failures:
- Etcd snapshot creation failures
- Storage space issues
- Backup upload failures
- Restore test failures

## Conclusion

Regular backup and restore testing is essential for maintaining RKE2 cluster resilience. Whether using Rancher's integrated tools or direct kubectl commands, having a robust backup strategy ensures you can recover from various failure scenarios. Always test your restore procedures regularly and keep backup configurations documented and version-controlled.